{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://colab.research.google.com/github/qayyumu/GradioExp/blob/main/session_03/Train_Custom_mdels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"gGiw79_dvMj0"},"source":["## Sertup environment"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13084,"status":"ok","timestamp":1720689878875,"user":{"displayName":"abbas khan","userId":"07434490259532895643"},"user_tz":-300},"id":"kbwwyHV5vM-s"},"outputs":[],"source":["!pip install transformers datasets gradio 2>/dev/null"]},{"cell_type":"markdown","metadata":{"id":"O35u_SDA0459"},"source":["## Text Summrization"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"96g3XqJw1ZGs"},"outputs":[],"source":["from transformers import pipeline\n","\n","get_completion = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n","\n","def summarize(input):\n","    output = get_completion(input)\n","    return output[0]['summary_text']"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"g2DMhtW61cEc"},"outputs":[],"source":["text = ('''The tower is 324 metres (1,063 ft) tall, about the same height\n","        as an 81-storey building, and the tallest structure in Paris.\n","        Its base is square, measuring 125 metres (410 ft) on each side.\n","        During its construction, the Eiffel Tower surpassed the Washington\n","        Monument to become the tallest man-made structure in the world,\n","        a title it held for 41 years until the Chrysler Building\n","        in New York City was finished in 1930. It was the first structure\n","        to reach a height of 300 metres. Due to the addition of a broadcasting\n","        aerial at the top of the tower in 1957, it is now taller than the\n","        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the\n","        Eiffel Tower is the second tallest free-standing structure in France\n","        after the Millau Viaduct.''')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. It is the second tallest free-standing structure in France after the Millau Viaduct.'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["summarize(text)"]},{"cell_type":"markdown","metadata":{"id":"p-mfcUK5uxw2"},"source":["# Text Classification"]},{"cell_type":"markdown","metadata":{"id":"VVRZBlwWvBND"},"source":["Text classification is a common NLP task that assigns a label or class to text. Some of the largest companies run text classification in production for a wide range of practical applications. One of the most popular forms of text classification is sentiment analysis, which assigns a label like üôÇ positive, üôÅ negative, or üòê neutral to a sequence of text.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"osndRtyCvYOc"},"source":["### Load IMDb dataset\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"b_BJyNHSvZU8"},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5377cfcd92e948ab86192b1d1600c2fd","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c508d301471e4aafa4f79e09f0867167","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb2fa9f4585a40ce89050d55c213ad92","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fa22bf90d7a4e3f829ad96bdf2dd59c","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f20278c6cb147468ab602611b1d798d","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"714b9fcb5b434a67af293f1ad69c773f","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61cd8caff3a04232b07818ff5c24acf7","version_major":2,"version_minor":0},"text/plain":["Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","imdb = load_dataset(\"imdb\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"lyRZV3TovcFH"},"outputs":[{"data":{"text/plain":["{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n"," 'label': 0}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["imdb[\"test\"][0]"]},{"cell_type":"markdown","metadata":{"id":"qnpG3LXovee7"},"source":["There are two fields in this dataset:\n","\n","* text: the movie review text.\n","\n","* label: a value that is either 0 for a negative review or 1 for a positive review."]},{"cell_type":"markdown","metadata":{"id":"DbluCJ3kvrIs"},"source":["### Preprocess\n","The next step is to load a DistilBERT tokenizer to preprocess the text field:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"0RVPCODNvq_N"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81ced0202ead46f38d830003ff8c0f2a","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/Users/anaconda3/envs/torchEnv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"265eff966aaa47319b0185844f2dafd4","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9355ff6a7524135816340c0842a7b5b","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5840e9718c0b4d65ac24f5a3a3117acb","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"]},{"cell_type":"markdown","metadata":{"id":"_XJ0QA-pvvd8"},"source":["Create a preprocessing function to tokenize text and truncate sequences to be no longer than DistilBERT‚Äôs maximum input length:\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"oeBg48Jlvm4k"},"outputs":[],"source":["def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"sUq2JivXvzKc"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4c60dbdce4c4cb8996c0607e356a48c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34b9545bb23348c3969a3a35482ddb7f","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50b7398ac299489e9774151aaea79120","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_imdb = imdb.map(preprocess_function, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"HA-pjVDKv9w4"},"source":["Now create a batch of examples using DataCollatorWithPadding. It‚Äôs more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"JHZwkn0rv-DB"},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"i4IxGv7LwL_z"},"source":["## Evaluate\n","Including a metric during training is often helpful for evaluating your model‚Äôs performance. You can quickly load a evaluation method with the ü§ó Evaluate library. For this task, load the accuracy metric (see the ü§ó Evaluate quick tour to learn more about how to load and compute a metric):"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"bTYQBgvgwEvx"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a0ebf87f8544e758bfc8f181dd9a834","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import evaluate\n","import numpy as np\n","\n","accuracy = evaluate.load(\"accuracy\")\n","\n","\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)\n","\n","id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n","label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"]},{"cell_type":"markdown","metadata":{"id":"S2aFABJrwRKM"},"source":["## Train Your own model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jumRWY-hwEmJ"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sm60Zo9Twi20"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"my_awesome_model\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_imdb[\"train\"],\n","    eval_dataset=tokenized_imdb[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"U32SIH-lwnEs"},"source":["### Inference\n","Great, now that you‚Äôve finetuned a model, you can use it for inference!\n","\n","Grab some text you‚Äôd like to run inference on:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8PAnVE5woI7"},"outputs":[],"source":["text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPjrAbN3wzXM"},"outputs":[],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\", model=\"stevhliu/my_awesome_model\")\n","classifier(text)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"torchEnv","language":"python","name":"torchenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}
